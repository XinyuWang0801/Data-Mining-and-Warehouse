{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "def fool_classifier(test_data): ## Please do not change the function defination...\n",
    "    ## Read the test data file, i.e., 'test_data.txt' from Present Working Directory...\n",
    "    \n",
    "    \n",
    "    ## You are supposed to use pre-defined class: 'strategy()' in the file `helper.py` for model training (if any),\n",
    "    #  and modifications limit checking\n",
    "    strategy_instance=helper.strategy() \n",
    "    #print('len(strategy_instance.class0)',len(strategy_instance.class0))\n",
    "    #print('len(strategy_instance.class1)',len(strategy_instance.class1))\n",
    "    #print('strategy_instance.class0:\\n',strategy_instance.class0[180])\n",
    "    #print('strategy_instance.class1:\\n',strategy_instance.class1[0])\n",
    "    x_train = strategy_instance.class0 + strategy_instance.class1\n",
    "    #print('len(x_train):',len(x_train))\n",
    "    #print('x_train:\\n',x_train[360])\n",
    "    import numpy as np\n",
    "    y = np.zeros((540,1),dtype=np.int)\n",
    "    #y[:361] = -1\n",
    "    y[360:] = 1 #这里好像错了 class0在前面 有360个 class1有180个  在后面 应该是y[360:] = 1\n",
    "    \n",
    "    y = y.ravel()\n",
    "    #print('y:\\n',y)\n",
    "#测试    \n",
    "    count_y1 = 0\n",
    "    for x in y:\n",
    "        if x == 1:\n",
    "            count_y1 += 1\n",
    "    #print('count_y1:',count_y1)\n",
    "#完毕\n",
    "    \n",
    "    def createVocabList(dataSet):# 找出class0 和class1中总共出现了哪些词汇，去重  建词汇表\n",
    "        vocabSet=set([])\n",
    "        for document in dataSet:\n",
    "            vocabSet=vocabSet|set(document)\n",
    "        return list(vocabSet)\n",
    "    \n",
    "    \n",
    "    def setOfWords2Vec(vocabSet,inputSet):#做一个len(字典list)大小的[0,0,0,0,……]  在inputSet中的每一个词，如果也在字典list中，对应位置标为1\n",
    "        returnVec=[0]*len(vocabSet)\n",
    "        for word in inputSet:\n",
    "            if word in vocabSet:\n",
    "                returnVec[vocabSet.index(word)] +=1\n",
    "        return returnVec\n",
    "    \n",
    "    data = createVocabList(x_train)# 一个list['have','like',……] 找出class0 和class1中总共出现了哪些词汇，去重  建词汇表\n",
    "    #print('data:\\n',data)\n",
    "    print('len(data):',len(data))\n",
    "    trainAll=[]\n",
    "    for postinDoc in x_train:\n",
    "        #print('postinDoc:',postinDoc)\n",
    "        trainAll.append(setOfWords2Vec(data,postinDoc))# 对 x_train的每个样例  返回该条样例中词出现在字典list中的位置  将位置的记录list加入trainALL\n",
    "    #test\n",
    "   # for i in range(len(trainAll)):\n",
    "    #    for x in trainAll[i]:\n",
    "     #       if x !=1 and x!=0:\n",
    "      #          #print('有不为1的')\n",
    "    #完毕\n",
    "    \n",
    "    trainAll = np.array(trainAll)\n",
    "    #print('trainALL:\\n',len(trainAll[0]))\n",
    "    with open('test_data.txt','r') as test1:\n",
    "        test1=[line.strip().split(' ') for line in test1]\n",
    "    \n",
    "    testAll=[]\n",
    "    for postinDoc in test1:\n",
    "        testAll.append(setOfWords2Vec(data,postinDoc))# 对test中每个样例处理 将样例中的词通过字典list位置映射起来\n",
    "    \n",
    "    parameters={} \n",
    "    parameters['C'] = 1\n",
    "    parameters['kernel'] = 'linear'\n",
    "    parameters['degree'] = 3\n",
    "    parameters['gamma'] = 'auto'\n",
    "    parameters['coef0'] = 1\n",
    "    clf = strategy_instance.train_svm(parameters,trainAll,y)\n",
    "#测试 test分类    \n",
    "    predict_test = clf.predict(testAll)\n",
    "    print('predict_test:\\n',predict_test)\n",
    "    #print('len(predict_test):\\n',len(predict_test))\n",
    "    count_1 = 0\n",
    "    for x in predict_test:\n",
    "        if x == 1:\n",
    "            count_1 += 1\n",
    "    print('count_1:',count_1)\n",
    "#   完毕\n",
    "#测试 class0 分类  \n",
    "    #c0 = strategy_instance.class0\n",
    "    #print('class0:',class0)\n",
    "    \n",
    "    \n",
    "    #c0All=[]\n",
    "    #for postinDoc in c0:\n",
    "        #print('postinDoc:',postinDoc)\n",
    "     #   c0All.append(setOfWords2Vec(data,postinDoc))# 对 x_train的每个样例  返回该条样例中词出现在字典list中的位置  将位置的记录list加入trainALL\n",
    "    #print('c0All:',len(c0All))\n",
    "    #predict_c0 = clf.predict(c0All)\n",
    "    #print('predict_c0:\\n',predict_c0)\n",
    "    #print('len(predict_c0):\\n',len(predict_c0))\n",
    "    #count_c01 = 0\n",
    "    #for x in predict_c0:\n",
    "     #   if x == 1:\n",
    "      #      count_c01 += 1\n",
    "    #print('count_c01:',count_c01)\n",
    "#   完毕\n",
    "#测试 class1 分类  \n",
    "  #  c1 = strategy_instance.class1\n",
    "    #print('class1:',class1)\n",
    "    \n",
    "    \n",
    "   # c1All=[]\n",
    "    #for postinDoc in c1:\n",
    "        #print('postinDoc:',postinDoc)\n",
    "     #   c1All.append(setOfWords2Vec(data,postinDoc))# 对 x_train的每个样例  返回该条样例中词出现在字典list中的位置  将位置的记录list加入trainALL\n",
    "    #print('c0All:',len(c1All))\n",
    "    #predict_c1 = clf.predict(c1All)\n",
    "    #print('predict_c1:\\n',predict_c1)\n",
    "    #print('len(predict_c1):\\n',len(predict_c1))\n",
    "    #count_c11 = 0\n",
    "    #for x in predict_c1:\n",
    "     #   if x == 1:\n",
    "      #      count_c11 += 1\n",
    "    #print('count_c11:',count_c11)\n",
    "#   完毕\n",
    "\n",
    "    w = clf.coef_\n",
    "    print('w',w)\n",
    "    print('len(w[0])',len(w[0]))\n",
    "    index = np.where(w[0] > 0)[0]\n",
    "    print('index:\\n',index)\n",
    "    index1 = np.where(w[0] > 0)\n",
    "    print('index1:\\n',index1)\n",
    "    \n",
    "    dic_w = {}\n",
    "    for i in index:\n",
    "        dic_w[i] = w[0][i]\n",
    "    print('前len(dic_w)',len(dic_w))\n",
    "    #print('前dic_w:\\n',dic_w)\n",
    "    \n",
    "    dic_w = sorted(dic_w.items(), key=lambda d: d[1],reverse=True)[0:200]\n",
    "    #print('\\ndic_w:\\n',dic_w)\n",
    "    print('后len(dic_w)',len(dic_w))\n",
    "    \n",
    "    index = [dic_w[i][0] for i in range(len(dic_w))]\n",
    "    #找出权重最大的200个  来比对进行删除\n",
    "    add_word = []\n",
    "    for i in index:\n",
    "        add_word.append(data[i])\n",
    "    #\n",
    "    print('add_word:\\n',add_word)\n",
    "    \n",
    "#判断存在哪20个\n",
    "    test2 = []\n",
    "    for i in range(len(test1)):\n",
    "        test2 += test1[i]\n",
    "    \n",
    "    del_w = []\n",
    "    for w in add_word:\n",
    "        for x in test2:\n",
    "            if x == w:\n",
    "                del_w.append(w)\n",
    "                break\n",
    "    del_word = del_w[:20] \n",
    "    print('del_w:\\n',del_w)\n",
    "    print('del_word\\n:',del_word)\n",
    "    print('len(del_word):',len(del_word))\n",
    "    \n",
    "    #删除20个  不会了\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "                \n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    file=open('./modified_data.txt','w')\n",
    "    for i in range(len(test1)):\n",
    "        file.write(\" \".join(test1[i]))\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "#测试modify    \n",
    "    with open('./modified_data.txt','r') as modified1:\n",
    "        modified1=[line.strip().split(' ') for line in modified1]\n",
    "    \n",
    "    modifiedAll=[]\n",
    "    for postinDoc in modified1:\n",
    "        modifiedAll.append(setOfWords2Vec(data,postinDoc))# 对test中每个样例处理 将样例中的词通过字典list位置映射起来\n",
    "    predict_modified = clf.predict(modifiedAll)\n",
    "    #print('predict_modified:\\n',predict_modified)\n",
    "    #print('len(predict_modified):\\n',len(predict_modified))\n",
    "    \n",
    "    count_m0 = 0\n",
    "    for x in predict_modified:\n",
    "        if x == 0:\n",
    "            count_m0 += 1\n",
    "    #print('count_m0:',count_m0)\n",
    "#完毕    \n",
    "    ## You can check that the modified text is within the modification limits.\n",
    "    modified_data='./modified_data.txt'\n",
    "    assert strategy_instance.check_data(test_data, modified_data)\n",
    "    return strategy_instance ## NOTE: You are required to return the instance of this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data): 5718\n",
      "predict_test:\n",
      " [1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
      " 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0\n",
      " 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1]\n",
      "count_1: 103\n",
      "w [[ 0.01326285  0.03641607  0.04227537 ... -0.01967724  0.01635086\n",
      "   0.0185369 ]]\n",
      "len(w[0]) 5718\n",
      "index:\n",
      " [   0    1    2 ... 5710 5716 5717]\n",
      "index1:\n",
      " (array([   0,    1,    2, ..., 5710, 5716, 5717], dtype=int64),)\n",
      "前len(dic_w) 2386\n",
      "后len(dic_w) 200\n",
      "add_word:\n",
      " ['mr.', 'president', '(', 'any', ',', 'about', 'russian', 'border', 'also', 'general', 'may', ')', 'chief', 'hold', 'come', '.', 'back', 'department', 'but', 'team', 'open', 'former', 'face', 'commander', 'sri', 'body', 'chavez', 'tell', 'pro', 'minister', 'olympic', 'hezbollah', 'confirm', 'compete', 'prosecutor', 'brazilian', 'games', 'storm', 'german', 'spokesman', 'along', 'ask', 'israeli', 'executive', 'without', 'robert', 'australia', 'possible', 'among', '6', 'jewish', 'rice', 'troop', 'yeltsin', 'exchange', 'press', 'abu', 'newspaper', 'stand', 'straight', '50', 'that', 'message', '%', 'cut', 'figure', 'view', 'prince', 'list', 'bulgaria', 'issue', 'muslims', 'his', 'house', 'policy', 'proposal', 'secretary', 'political', 'pull', 'since', 'project', 'opposition', '18', 'afghan', 'clash', 'federal', 'aim', 'widely', 'power', 'step', 'austria', 'peace', 'global', 'autonomy', 'reduce', 'hugo', 'chris', 'australian', 'early', 'replacement', 'operate', 'current', 'conference', 'church', 'thursday', 'civil', 'cypriot', 'deny', 'david', 'include', 'philippine', 'bhutan', 'court', 'society', 'restore', 'prime', 'bhutto', 'lap', 'year', 'obama', 'powerful', 'research', 'commission', 'hop', 'easily', 'takeda', 'arab', 'league', 'detain', '10', 'secret', 'extend', 'legal', 'appear', 'bali', 'gyanendra', 'win', 'die', 'upcom', 'powell', 'high', 'threat', 'benedict', 'sixth', 'leadership', 'major', 'dialogue', 'netherlands', 'million', 'justice', 'mubarak', 'inning', 'putin', 'tehran', 'candidate', 'system', 'appoint', 'military', 'mountain', 'gunfight', 'negotiator', 'decide', 'music', 'judge', 'cite', 'real', 'ambassador', 'strength', 'wang', 'place', 'second', 'home', 'renew', 'medical', 'kilometer', 'release', 'wimbledon', 'we', 'norwegian', 'standing', 'hit', 'although', 'several', 'walid', 'arrive', 'plo', 'intend', 'nepal', 'palestinians', 'squad', 'convict', 'wound', 'pass', 'important', 'block', 'lanka', 'surge', 'king', 'paul', 'ill']\n",
      "del_w:\n",
      " ['mr.', 'president', '(', 'any', ',', 'about', 'russian', 'border', 'also', 'general', 'may', ')', 'chief', 'hold', 'come', '.', 'back', 'department', 'but', 'team', 'open', 'former', 'face', 'commander', 'sri', 'body', 'chavez', 'tell', 'pro', 'minister', 'olympic', 'confirm', 'prosecutor', 'brazilian', 'games', 'storm', 'german', 'spokesman', 'along', 'ask', 'israeli', 'executive', 'without', 'robert', 'australia', 'possible', 'among', '6', 'jewish', 'rice', 'troop', 'exchange', 'press', 'abu', 'newspaper', 'stand', 'straight', '50', 'that', 'message', '%', 'cut', 'figure', 'view', 'list', 'bulgaria', 'issue', 'muslims', 'his', 'house', 'policy', 'proposal', 'secretary', 'political', 'pull', 'since', 'project', 'opposition', '18', 'afghan', 'clash', 'federal', 'aim', 'power', 'step', 'austria', 'peace', 'global', 'reduce', 'hugo', 'australian', 'early', 'operate', 'current', 'conference', 'church', 'thursday', 'civil', 'cypriot', 'deny', 'david', 'include', 'philippine', 'court', 'restore', 'prime', 'bhutto', 'year', 'obama', 'research', 'commission', 'hop', 'arab', 'league', 'detain', '10', 'secret', 'extend', 'legal', 'appear', 'bali', 'win', 'die', 'high', 'threat', 'sixth', 'leadership', 'major', 'netherlands', 'million', 'justice', 'mubarak', 'inning', 'putin', 'tehran', 'candidate', 'system', 'appoint', 'military', 'negotiator', 'decide', 'music', 'judge', 'cite', 'real', 'ambassador', 'place', 'second', 'home', 'renew', 'medical', 'kilometer', 'release', 'we', 'standing', 'hit', 'although', 'several', 'arrive', 'intend', 'palestinians', 'squad', 'convict', 'wound', 'pass', 'important', 'surge', 'king', 'paul']\n",
      "del_word\n",
      ": ['mr.', 'president', '(', 'any', ',', 'about', 'russian', 'border', 'also', 'general', 'may', ')', 'chief', 'hold', 'come', '.', 'back', 'department', 'but', 'team']\n",
      "len(del_word): 20\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-30ff9e781468>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./test_data.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstrategy_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfool_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-cfeb8a71a560>\u001b[0m in \u001b[0;36mfool_classifier\u001b[1;34m(test_data)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;31m## You can check that the modified text is within the modification limits.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mmodified_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./modified_data.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mstrategy_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodified_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstrategy_instance\u001b[0m \u001b[1;31m## NOTE: You are required to return the instance of this class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\9318-Project\\try\\helper.py\u001b[0m in \u001b[0;36mcheck_data\u001b[1;34m(self, original_file, modified_file)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOriginal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModified\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data='./test_data.txt'\n",
    "strategy_instance = fool_classifier(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 4, 5]\n",
      "[6, 7, 8, 9, 3, 4, 2]\n",
      "[1, 2, 3, 4, 3, 2, 1, 2, 3]\n",
      "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 3, 4], [1, 3, 4, 3, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,2,2,3,4,5],[6,7,8,9,3,4,2],[1,2,3,4,3,2,1,2,3]]\n",
    "word = [1,2,3]\n",
    "w=2\n",
    "for i in range(len(a)):\n",
    "    \n",
    "    for x in a[i]:\n",
    "        if x==w:\n",
    "            a[i].remove(x)\n",
    "    \n",
    "        \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
